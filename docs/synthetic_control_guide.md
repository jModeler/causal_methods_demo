# Synthetic Control Method Guide

A comprehensive guide to understanding and applying the Synthetic Control method for causal inference in business contexts.

## üéØ **What is Synthetic Control?**

**Synthetic Control** is a causal inference method that constructs artificial control units by taking weighted combinations of donor units. Instead of using a single control group, synthetic control creates a "synthetic" control unit that closely matches each treated unit's pre-treatment characteristics.

### **Core Concept**

For each treated unit, we find optimal weights $w_{ij}$ for donor units such that:

$$\text{Synthetic Control}_i = \sum_{j \in \text{Donors}} w_{ij} \cdot \text{Donor}_j$$

Where weights are optimized to minimize:
$$\min_W \sum_{k=1}^K (X_{1k} - \sum_{j=2}^{J+1} w_j X_{jk})^2$$

Subject to: $\sum_j w_j = 1$ and $w_j \geq 0$ for all $j$.

## üîç **When to Use Synthetic Control**

### **‚úÖ Ideal Scenarios**
- **Rich pre-treatment data**: Multiple relevant characteristics available
- **Individual-level effects needed**: Want to see treatment heterogeneity
- **Observational studies**: Treatment assignment not randomized
- **Transparent methodology**: Need to explain which units contribute to comparisons
- **Multiple time periods**: Both pre and post-treatment data available

### **‚ùå Avoid When**
- **Limited pre-treatment data**: Few predictors available
- **Very small samples**: Insufficient donor pool for good matching
- **Treatment spillovers**: Treatment of one unit affects others
- **Unstable relationships**: Pre-treatment predictors don't predict post-treatment outcomes

## üßÆ **Key Assumptions**

### **1. Rich Pre-treatment Characteristics**
- Multiple relevant predictors of the outcome are available
- Predictors capture important determinants of treatment assignment and outcomes

### **2. No Anticipation Effects**
- Units didn't change behavior in anticipation of treatment
- Pre-treatment period represents "normal" behavior

### **3. Stable Relationships**
- Relationships between predictors and outcomes remain stable over time
- What predicts outcomes pre-treatment continues to predict post-treatment

### **4. No Spillover Effects**
- Treatment of one unit doesn't affect outcomes of other units
- Donor units remain unaffected by treated units

### **5. Convex Hull Condition**
- Treated units' characteristics lie within the convex hull of donor units
- Synthetic controls can be constructed through positive weighted combinations

## üìä **Business Applications**

### **Marketing Campaign Evaluation**
```python
# Evaluate impact of targeted marketing on customer segments
sc = SyntheticControl(customer_data)
results = sc.construct_synthetic_controls(
    outcome_pre_col='revenue_2023',
    outcome_post_col='revenue_2024',
    predictor_cols=['past_revenue', 'engagement_score', 'demographics']
)
```

### **Product Feature Impact**
```python
# Assess new feature adoption on user behavior
sc = SyntheticControl(user_data)
results = sc.construct_synthetic_controls(
    outcome_pre_col='usage_before',
    outcome_post_col='usage_after',
    predictor_cols=['historical_usage', 'user_type', 'platform']
)
```

### **Policy Implementation Analysis**
```python
# Evaluate policy change effects across business units
sc = SyntheticControl(business_unit_data)
results = sc.construct_synthetic_controls(
    outcome_pre_col='performance_before',
    outcome_post_col='performance_after',
    predictor_cols=['historical_performance', 'size', 'region']
)
```

## üõ†Ô∏è **Implementation Workflow**

### **Step 1: Data Preparation**
```python
from src.causal_methods.synthetic_control import SyntheticControl

# Initialize with your dataset
sc = SyntheticControl(df, random_state=42)

# Check data structure
data_prep = sc.prepare_data(
    unit_id_col='user_id',
    treatment_col='received_treatment',
    outcome_pre_col='outcome_before',
    outcome_post_col='outcome_after',
    predictor_cols=['predictor1', 'predictor2', 'predictor3']
)

print(f"Treated units: {data_prep['n_treated']}")
print(f"Donor pool size: {data_prep['n_donors']}")
```

### **Step 2: Construct Synthetic Controls**
```python
# Build synthetic controls with standardization
results = sc.construct_synthetic_controls(
    unit_id_col='user_id',
    treatment_col='received_treatment',
    outcome_pre_col='outcome_before',
    outcome_post_col='outcome_after',
    predictor_cols=['predictor1', 'predictor2', 'predictor3'],
    standardize=True  # Recommended for fair weighting
)

# Extract key results
ate = results['average_treatment_effect']
se = results['ate_std_error']
individual_effects = results['individual_results']

print(f"Average Treatment Effect: {ate:.4f} ¬± {se:.4f}")
```

### **Step 3: Quality Assessment**
```python
# Assess synthetic control quality
print("Quality Metrics:")
print(f"Pre-treatment fit error: {results['average_pre_treatment_error']:.4f}")
print(f"Weight concentration: {results['weight_concentration']:.4f}")

# Good quality indicators:
# - Pre-treatment error < 0.1 (excellent < 0.05)
# - Weight concentration < 0.5 (not too concentrated)

if results['average_pre_treatment_error'] < 0.05:
    print("‚úÖ Excellent pre-treatment fit")
elif results['average_pre_treatment_error'] < 0.1:
    print("‚úÖ Good pre-treatment fit")
else:
    print("‚ö†Ô∏è  Fair pre-treatment fit - interpret with caution")
```

### **Step 4: Statistical Significance Testing**
```python
# Run placebo tests for statistical inference
placebo_results = sc.estimate_statistical_significance(n_placebo=50)

p_value = placebo_results['p_value']
observed_ate = placebo_results['observed_ate']

print(f"Observed ATE: {observed_ate:.4f}")
print(f"P-value: {p_value:.4f}")

if p_value < 0.05:
    print("‚úÖ Statistically significant effect")
else:
    print("‚ùå Effect not statistically significant")
```

### **Step 5: Visualization and Reporting**
```python
# Create comprehensive visualizations
fig = sc.plot_treatment_effects(figsize=(16, 10))
plt.show()

# Generate business report
report = sc.generate_summary_report()
print(report)
```

## üìà **Interpreting Results**

### **Average Treatment Effect (ATE)**
```python
# Main causal estimate
ate = results['average_treatment_effect']
se = results['ate_std_error']

# Confidence interval
ci_lower = ate - 1.96 * se
ci_upper = ate + 1.96 * se

print(f"ATE: {ate:.4f}")
print(f"95% CI: [{ci_lower:.4f}, {ci_upper:.4f}]")

# Business interpretation
if ate > 0.02:  # 2 percentage points for binary outcomes
    print("üöÄ Substantial positive effect")
elif ate > 0:
    print("‚úÖ Positive effect")
elif ate < -0.02:
    print("‚ùå Substantial negative effect")
else:
    print("‚ûñ Minimal effect")
```

### **Individual Treatment Effects**
```python
# Examine heterogeneity
individual_df = results['individual_results']

# Distribution of effects
print("Individual Treatment Effects:")
print(f"Min: {individual_df['treatment_effect'].min():.4f}")
print(f"25th percentile: {individual_df['treatment_effect'].quantile(0.25):.4f}")
print(f"Median: {individual_df['treatment_effect'].median():.4f}")
print(f"75th percentile: {individual_df['treatment_effect'].quantile(0.75):.4f}")
print(f"Max: {individual_df['treatment_effect'].max():.4f}")

# Count positive vs negative effects
positive_effects = (individual_df['treatment_effect'] > 0).sum()
negative_effects = (individual_df['treatment_effect'] < 0).sum()
total_units = len(individual_df)

print(f"Positive effects: {positive_effects}/{total_units} ({positive_effects/total_units:.1%})")
print(f"Negative effects: {negative_effects}/{total_units} ({negative_effects/total_units:.1%})")
```

### **Quality Metrics**
```python
# Pre-treatment fit quality
pre_error = results['average_pre_treatment_error']
if pre_error < 0.05:
    fit_quality = "Excellent"
elif pre_error < 0.1:
    fit_quality = "Good"
else:
    fit_quality = "Fair"

print(f"Pre-treatment fit: {fit_quality} (error: {pre_error:.4f})")

# Weight concentration analysis
weight_concentration = results['weight_concentration']
if weight_concentration < 0.3:
    weight_quality = "Well-dispersed"
elif weight_concentration < 0.5:
    weight_quality = "Moderately concentrated"
else:
    weight_quality = "Highly concentrated"

print(f"Weight distribution: {weight_quality} (concentration: {weight_concentration:.4f})")
```

## üîç **Diagnostic Checks**

### **1. Pre-treatment Fit Assessment**
```python
# Check how well synthetic controls match treated units pre-treatment
individual_df = results['individual_results']

# Plot actual vs synthetic pre-treatment outcomes
import matplotlib.pyplot as plt

plt.figure(figsize=(10, 6))
plt.scatter(individual_df['actual_pre'], individual_df['synthetic_pre'], alpha=0.6)

# Add perfect fit line
min_val = min(individual_df['actual_pre'].min(), individual_df['synthetic_pre'].min())
max_val = max(individual_df['actual_pre'].max(), individual_df['synthetic_pre'].max())
plt.plot([min_val, max_val], [min_val, max_val], 'r--', label='Perfect Fit')

plt.xlabel('Actual Pre-treatment Outcome')
plt.ylabel('Synthetic Pre-treatment Outcome')
plt.title('Pre-treatment Fit Quality')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()

# Calculate correlation
correlation = individual_df['actual_pre'].corr(individual_df['synthetic_pre'])
print(f"Pre-treatment correlation: {correlation:.3f}")
```

### **2. Weight Analysis**
```python
# Examine synthetic control weights
import numpy as np

all_weights = np.array(list(sc.synthetic_weights.values()))

# Weight statistics
print("Weight Analysis:")
print(f"Average weight: {all_weights.mean():.4f}")
print(f"Max weight: {all_weights.max():.4f}")
print(f"Weights > 0.01: {(all_weights > 0.01).sum()} out of {all_weights.size}")

# Identify most frequently used donors
donor_usage = (all_weights > 0.01).sum(axis=0)
most_used_donor_idx = np.argmax(donor_usage)
print(f"Most used donor: Used in {donor_usage[most_used_donor_idx]} synthetic controls")

# Check for overly concentrated weights
high_concentration_units = (all_weights.max(axis=1) > 0.5).sum()
print(f"Units with concentrated weights (>50% on one donor): {high_concentration_units}")
```

### **3. Placebo Test Diagnostics**
```python
# Analyze placebo test results
placebo_effects = placebo_results['placebo_effects']
observed_ate = placebo_results['observed_ate']

# Distribution of placebo effects
print("Placebo Test Diagnostics:")
print(f"Placebo effects mean: {placebo_effects.mean():.4f}")
print(f"Placebo effects std: {placebo_effects.std():.4f}")
print(f"Observed effect rank: {np.sum(placebo_effects <= observed_ate)} out of {len(placebo_effects)}")

# Visualize placebo distribution
plt.figure(figsize=(10, 6))
plt.hist(placebo_effects, bins=20, alpha=0.7, color='lightgray', 
         edgecolor='black', label='Placebo Effects')
plt.axvline(observed_ate, color='red', linestyle='--', linewidth=2,
           label=f'Observed Effect: {observed_ate:.4f}')
plt.xlabel('Treatment Effect')
plt.ylabel('Frequency')
plt.title('Placebo Test Results')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()
```

## ‚ö†Ô∏è **Common Pitfalls and Solutions**

### **1. Poor Pre-treatment Fit**
**Problem**: High pre-treatment error indicates poor synthetic control quality.

**Solutions**:
- Add more relevant predictors
- Check for outliers in treated units
- Consider removing treated units with very poor fit
- Verify data quality and transformations

```python
# Identify units with poor fit
poor_fit_threshold = 0.2
poor_fit_units = individual_df[individual_df['pre_treatment_error'] > poor_fit_threshold]
print(f"Units with poor fit: {len(poor_fit_units)}")

# Consider excluding poor fit units
if len(poor_fit_units) > 0:
    print("Consider investigating these units or improving predictors")
```

### **2. Highly Concentrated Weights**
**Problem**: Synthetic controls rely too heavily on few donors.

**Solutions**:
- Expand donor pool if possible
- Add more predictors for better matching
- Consider robustness checks excluding high-weight donors

```python
# Check weight concentration
high_weight_threshold = 0.5
concentrated_units = (all_weights.max(axis=1) > high_weight_threshold).sum()
print(f"Units with concentrated weights: {concentrated_units}")

# Robustness check: exclude high-weight donors
if concentrated_units > len(individual_df) * 0.2:  # >20% of units
    print("‚ö†Ô∏è  Consider robustness checks or expanding donor pool")
```

### **3. Insufficient Donor Pool**
**Problem**: Too few control units for reliable matching.

**Solutions**:
- Expand data collection if possible
- Consider relaxing inclusion criteria
- Use alternative methods (PSM, DML) if donor pool too small

```python
# Check donor pool adequacy
n_treated = data_prep['n_treated']
n_donors = data_prep['n_donors']
donor_ratio = n_donors / n_treated

print(f"Donor-to-treated ratio: {donor_ratio:.1f}:1")

if donor_ratio < 3:
    print("‚ö†Ô∏è  Small donor pool - consider expanding or alternative methods")
elif donor_ratio < 10:
    print("‚úÖ Adequate donor pool")
else:
    print("‚úÖ Large donor pool - good for robust matching")
```

## üìä **Business Impact Translation**

### **ROI Calculation**
```python
# Calculate business impact
def calculate_business_impact(ate, se, user_base, revenue_per_conversion):
    """Calculate business impact with confidence intervals."""
    
    # Point estimates
    additional_conversions = user_base * ate
    revenue_impact = additional_conversions * revenue_per_conversion
    
    # Confidence intervals
    ci_lower = user_base * (ate - 1.96 * se) * revenue_per_conversion
    ci_upper = user_base * (ate + 1.96 * se) * revenue_per_conversion
    
    return {
        'additional_conversions': additional_conversions,
        'revenue_impact': revenue_impact,
        'ci_lower': ci_lower,
        'ci_upper': ci_upper
    }

# Example calculation
impact = calculate_business_impact(
    ate=0.04,  # 4 percentage point increase
    se=0.01,   # Standard error
    user_base=100000,  # 100K users
    revenue_per_conversion=50  # $50 per conversion
)

print(f"Business Impact Analysis:")
print(f"Additional conversions: {impact['additional_conversions']:,.0f}")
print(f"Revenue impact: ${impact['revenue_impact']:,.0f}")
print(f"95% CI: ${impact['ci_lower']:,.0f} to ${impact['ci_upper']:,.0f}")
```

### **Segment Analysis**
```python
# Analyze treatment effects by segments
def analyze_by_segments(individual_df, segment_col):
    """Analyze treatment effects by user segments."""
    
    segment_analysis = individual_df.groupby(segment_col)['treatment_effect'].agg([
        'count', 'mean', 'std', 'min', 'max'
    ]).round(4)
    
    return segment_analysis

# Example: analyze by user segment
if 'user_segment' in individual_df.columns:
    segment_results = analyze_by_segments(individual_df, 'user_segment')
    print("Treatment Effects by Segment:")
    print(segment_results)
```

## üîÑ **Method Comparison**

### **Synthetic Control vs Other Methods**

| Aspect | Synthetic Control | PSM | DML | DiD | CUPED |
|--------|------------------|-----|-----|-----|-------|
| **Individual Effects** | ‚úÖ Yes | ‚úÖ Yes | ‚úÖ Yes | ‚ùå No | ‚ùå No |
| **Transparency** | ‚úÖ High | ‚úÖ High | ‚ùå Low | ‚úÖ Medium | ‚úÖ High |
| **Assumptions** | ‚úÖ Minimal | ‚ö†Ô∏è Strong | ‚ö†Ô∏è Complex | ‚ö†Ô∏è Strong | ‚úÖ Minimal |
| **Data Requirements** | ‚ö†Ô∏è Rich pre-data | ‚úÖ Moderate | ‚ö†Ô∏è High-dim | ‚ö†Ô∏è Panel data | ‚úÖ Pre-experiment |
| **Sample Size** | ‚ö†Ô∏è Medium-Large | ‚úÖ Flexible | ‚úÖ Large | ‚úÖ Flexible | ‚úÖ Any |

### **When to Prefer Synthetic Control**
- **Individual-level insights needed**: Want to see treatment heterogeneity
- **Transparent methodology required**: Need to explain matching process
- **Rich pre-treatment data available**: Multiple relevant predictors
- **No parametric assumptions desired**: Avoid distributional assumptions
- **Observational study context**: Treatment not randomized

## üöÄ **Advanced Features**

### **Custom Predictor Selection**
```python
# Select predictors based on correlation with outcome
def select_predictors_by_correlation(df, outcome_col, candidate_cols, threshold=0.1):
    """Select predictors based on correlation with outcome."""
    
    correlations = {}
    for col in candidate_cols:
        if col in df.columns:
            corr = abs(df[col].corr(df[outcome_col]))
            if corr > threshold:
                correlations[col] = corr
    
    # Sort by correlation strength
    selected = sorted(correlations.keys(), key=correlations.get, reverse=True)
    return selected

# Example usage
candidate_predictors = ['age', 'income', 'past_behavior', 'engagement_score']
selected_predictors = select_predictors_by_correlation(
    df, 'outcome_2024', candidate_predictors, threshold=0.05
)
print(f"Selected predictors: {selected_predictors}")
```

### **Robustness Checks**
```python
# Run synthetic control with different predictor sets
def robustness_check(sc, predictor_sets):
    """Test robustness across different predictor combinations."""
    
    results = {}
    for name, predictors in predictor_sets.items():
        try:
            result = sc.construct_synthetic_controls(predictor_cols=predictors)
            results[name] = {
                'ate': result['average_treatment_effect'],
                'se': result['ate_std_error'],
                'pre_error': result['average_pre_treatment_error']
            }
        except Exception as e:
            results[name] = {'error': str(e)}
    
    return results

# Example robustness check
predictor_sets = {
    'baseline': ['filed_2023', 'age'],
    'extended': ['filed_2023', 'age', 'tech_savviness'],
    'full': ['filed_2023', 'age', 'tech_savviness', 'sessions_2023']
}

robustness_results = robustness_check(sc, predictor_sets)
for name, result in robustness_results.items():
    if 'ate' in result:
        print(f"{name}: ATE={result['ate']:.4f}, SE={result['se']:.4f}")
```

## üéì **Best Practices**

### **Data Preparation**
1. **Standardize predictors** for fair weighting
2. **Handle missing values** appropriately
3. **Check for outliers** that might distort matching
4. **Include relevant predictors** that predict both treatment and outcome

### **Analysis Workflow**
1. **Start with exploratory analysis** to understand data structure
2. **Check assumption plausibility** before running main analysis
3. **Assess quality metrics** before interpreting results
4. **Run placebo tests** for statistical inference
5. **Conduct robustness checks** with different specifications

### **Interpretation Guidelines**
1. **Focus on quality metrics** - poor fit undermines results
2. **Consider individual heterogeneity** - don't just report averages
3. **Translate to business metrics** - make results actionable
4. **Acknowledge limitations** - be transparent about assumptions

### **Reporting Standards**
1. **Document all specifications** used in analysis
2. **Report quality diagnostics** alongside main results
3. **Include confidence intervals** and significance tests
4. **Provide business interpretation** with impact projections

## üìö **Further Reading**

### **Academic References**
- Abadie, A., Diamond, A., & Hainmueller, J. (2010). "Synthetic Control Methods for Comparative Case Studies"
- Abadie, A., & Gardeazabal, J. (2003). "The Economic Costs of Conflict: A Case Study of the Basque Country"
- Athey, S., & Imbens, G. W. (2017). "The State of Applied Econometrics: Causality and Policy Evaluation"

### **Methodological Extensions**
- **Matrix Completion Methods**: For missing data in panel settings
- **Bayesian Synthetic Control**: Incorporating prior information
- **Robust Synthetic Control**: Handling outliers and contamination
- **Generalized Synthetic Control**: Relaxing convex hull constraints

### **Software Implementations**
- **R**: `Synth` package, `gsynth` package
- **Python**: This implementation, `SyntheticControlMethods` package
- **Stata**: `synth` command

---

**üéØ Synthetic Control provides a powerful, transparent approach to causal inference that's particularly valuable when you need individual-level treatment effects and want to understand exactly how your comparisons are constructed.** 